import axios from "axios";
import pLimit from "p-limit";
import {
  B2SmartPresignedUrl,
  B2CompleteMultipart,
  B2AbortMultipart,
  failedDeleteImagesFromB2,
} from "../redux/features/album/albumThunks";
import { realSpeedDetector } from "./networkSpeedDetector";

const ORPHAN_STORAGE_KEY = "B2_orphan_files";

// ðŸ”¥ FILE SIZE THRESHOLDS (in MB)
const UPLOAD_THRESHOLDS = {
  DIRECT_MAX: 10, // 0-10MB: Direct PUT
  CHUNKED_MAX: 30, // 10-30MB: 5MB chunks
  MULTIPART_MAX: 60, // 30-60MB: 10MB chunks
};

/**
 * ðŸ”¥ SMART FILE UPLOADER
 * Automatically chooses best upload strategy based on file size
 */
class SmartFileUploader {
  constructor(file, path, type, progressCallback) {
    this.file = file;
    this.path = path;
    this.type = type;
    this.progressCallback = progressCallback;
    this.fileSizeMB = file.size / (1024 * 1024);
    this.uploadedBytes = 0;
    this.lastReportedBytes = 0;
    this.isCancelled = false;
  }

  async upload() {
    try {
      // ðŸ”¥ STRATEGY 1: Direct Upload (0-10MB)
      if (this.fileSizeMB <= UPLOAD_THRESHOLDS.DIRECT_MAX) {
        return await this.directUpload();
      }

      // ðŸ”¥ STRATEGY 2 & 3: Multipart Upload (10MB+)
      return await this.multipartUpload();
    } catch (error) {
      console.error(`Upload failed for ${this.path}:`, error);
      throw error;
    }
  }

  /**
   * Direct PUT upload for small files (<10MB)
   */
  async directUpload() {
    try {
      // Get presigned URL
      const response = await B2SmartPresignedUrl({
        fileType: this.type,
        folderPath: this.path,
        fileSize: this.file.size,
      });

      if (!response?.success || !response?.uploadUrl) {
        throw new Error("Failed to get upload URL");
      }

      const { uploadUrl } = response;

      // Upload with progress tracking
      await axios.put(uploadUrl, this.file, {
        headers: { "Content-Type": this.type },
        timeout: 5 * 60 * 1000, // 5 minutes
        maxContentLength: Infinity,
        maxBodyLength: Infinity,
        onUploadProgress: (e) => {
          if (this.progressCallback && !this.isCancelled) {
            const progress = Math.round((e.loaded * 100) / e.total);
            const bytesLoaded = e.loaded - this.lastReportedBytes;
            this.lastReportedBytes = e.loaded;
            this.progressCallback(progress, bytesLoaded);
          }
        },
      });

      return { success: true, method: "direct", path: this.path };
    } catch (error) {
      console.error(`âŒ Direct upload failed: ${this.path}`, error);
      throw error;
    }
  }

  /**
   * Multipart upload for large files (>10MB)
   */
  async multipartUpload() {
    let uploadId = null;
    const uploadedParts = [];

    try {
      // ðŸ”¥ STEP 1: Initialize multipart upload
      const response = await B2SmartPresignedUrl({
        fileType: this.type,
        folderPath: this.path,
        fileSize: this.file.size,
      });

      if (!response?.success || !response?.uploadId) {
        throw new Error("Failed to initialize multipart upload");
      }

      const { uploadId: id, partUrls, chunkSize, totalParts } = response;
      uploadId = id;

      // ðŸ”¥ STEP 2: Upload parts sequentially (with retry)
      for (let i = 0; i < partUrls.length; i++) {
        if (this.isCancelled) {
          throw new Error("Upload cancelled by user");
        }

        const { partNumber, uploadUrl } = partUrls[i];
        const start = (partNumber - 1) * chunkSize;
        const end = Math.min(start + chunkSize, this.file.size);
        const chunk = this.file.slice(start, end);

        // Upload part with retry
        const etag = await this.uploadPart(
          uploadUrl,
          chunk,
          partNumber,
          totalParts,
          chunkSize,
        );

        uploadedParts.push({ partNumber, etag });
      }

      // ðŸ”¥ STEP 3: Complete multipart upload
      const completeResponse = await B2CompleteMultipart({
        folderPath: this.path,
        uploadId,
        parts: uploadedParts,
      });

      if (!completeResponse?.success) {
        throw new Error("Failed to complete multipart upload");
      }

      return {
        success: true,
        method: "multipart",
        parts: totalParts,
        path: this.path,
      };
    } catch (error) {
      console.error(`âŒ Multipart upload failed: ${this.path}`, error);

      // ðŸ”¥ Cleanup: Abort multipart upload
      if (uploadId) {
        try {
          await B2AbortMultipart({
            folderPath: this.path,
            uploadId,
          });
        } catch (abortError) {
          console.error("Failed to abort multipart:", abortError);
        }
      }

      throw error;
    }
  }

  /**
   * Upload single part with retry logic
   */
  async uploadPart(uploadUrl, chunk, partNumber, totalParts, chunkSize) {
    const maxRetries = 3;
    let lastError;

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        const partStartBytes = this.uploadedBytes;

        const response = await axios.put(uploadUrl, chunk, {
          headers: { "Content-Type": "application/octet-stream" },
          timeout: 5 * 60 * 1000, // 5 minutes per part
          maxContentLength: Infinity,
          maxBodyLength: Infinity,
          onUploadProgress: (e) => {
            if (this.progressCallback && !this.isCancelled) {
              // Calculate overall progress
              const partProgress = e.loaded / chunk.size;
              const overallProgress =
                ((partNumber - 1 + partProgress) / totalParts) * 100;

              const currentBytes = partStartBytes + e.loaded;
              const bytesLoaded = currentBytes - this.lastReportedBytes;

              if (bytesLoaded > 0) {
                this.lastReportedBytes = currentBytes;
                this.progressCallback(Math.round(overallProgress), bytesLoaded);
              }
            }
          },
        });

        // Update total uploaded bytes for this part
        this.uploadedBytes += chunk.size;

        // Extract ETag from response headers
        const etag = response.headers.etag?.replace(/"/g, "");

        if (!etag) {
          throw new Error(`No ETag in response for part ${partNumber}`);
        }

        return etag;
      } catch (error) {
        lastError = error;

        if (attempt < maxRetries) {
          const delay = Math.min(1000 * Math.pow(2, attempt - 1), 5000);
          console.warn(
            `âš ï¸ Retry part ${partNumber} (attempt ${attempt}/${maxRetries}) in ${delay}ms`,
          );
          await new Promise((resolve) => setTimeout(resolve, delay));
        }
      }
    }

    throw lastError;
  }

  cancel() {
    this.isCancelled = true;
  }
}

// ðŸ”¥ ENHANCED Worker Pool with proper worker tracking
class WorkerPool {
  constructor(workerScript, poolSize = navigator.hardwareConcurrency || 4) {
    this.workers = [];
    this.queue = [];
    this.activeJobs = new Map();
    this.jobIdCounter = 0;
    this.processingTimeout = 60000;

    for (let i = 0; i < poolSize; i++) {
      const worker = new Worker(workerScript);
      worker.onmessage = (e) => this.handleMessage(e, i);
      worker.onerror = (e) => this.handleError(e, i);

      this.workers.push({
        worker,
        busy: false,
        id: i,
      });
    }
  }

  processImage(file, hash, createThumbnail = true, originalImg = false) {
    return new Promise((resolve, reject) => {
      const availableWorker = this.workers.find((w) => !w.busy);
      const jobId = this.jobIdCounter++;

      const job = {
        resolve,
        reject,
        hash,
        jobId,
        fileName: file.name,
        fileSize: (file.size / 1024 / 1024).toFixed(2) + "MB",
        timeoutId: null,
      };

      if (availableWorker) {
        this.startJob(
          availableWorker,
          file,
          hash,
          createThumbnail,
          originalImg,
          job,
        );
      } else {
        this.queue.push({ file, hash, createThumbnail, originalImg, ...job });
      }
    });
  }

  startJob(worker, file, hash, createThumbnail, originalImg, job) {
    worker.busy = true;
    this.activeJobs.set(worker.id, job);

    // Add timeout to prevent stuck jobs
    job.timeoutId = setTimeout(() => {
      console.error(`âš ï¸ Job timeout: ${job.fileName} (${job.fileSize})`);
      this.handleJobTimeout(worker.id);
    }, this.processingTimeout);

    worker.worker.postMessage({
      file,
      hash,
      createThumbnail,
      originalImg,
      jobId: job.jobId,
    });
  }

  handleMessage(e, workerId) {
    const { success, result, error, hash, jobId } = e.data;
    const worker = this.workers[workerId];
    const job = this.activeJobs.get(workerId);

    if (!job || !worker) {
      console.error(`âš ï¸ Worker ${workerId} returned orphan result`);
      return;
    }

    // Clear timeout
    if (job.timeoutId) {
      clearTimeout(job.timeoutId);
    }

    // Verify hash
    if (job.hash !== hash) {
      console.error(`âŒ Hash mismatch! Expected ${job.hash}, got ${hash}`);
      job.reject(new Error(`Hash mismatch for ${job.fileName}`));
      this.freeWorker(workerId);
      return;
    }

    worker.busy = false;
    this.activeJobs.delete(workerId);

    if (success) {
      job.resolve(result);
    } else {
      job.reject(new Error(error));
    }

    // Process next in queue
    this.processNextInQueue(workerId);
  }

  handleError(error, workerId) {
    console.error(`âŒ Worker ${workerId} error:`, error);
    const job = this.activeJobs.get(workerId);

    if (job) {
      if (job.timeoutId) clearTimeout(job.timeoutId);
      job.reject(new Error(`Worker error: ${error.message}`));
      this.freeWorker(workerId);
    }
  }

  handleJobTimeout(workerId) {
    const job = this.activeJobs.get(workerId);
    if (job) {
      job.reject(new Error(`Processing timeout for ${job.fileName}`));
      this.freeWorker(workerId);
    }
  }

  freeWorker(workerId) {
    const worker = this.workers[workerId];
    if (worker) {
      worker.busy = false;
      this.activeJobs.delete(workerId);
      this.processNextInQueue(workerId);
    }
  }

  processNextInQueue(workerId) {
    if (this.queue.length > 0) {
      const nextJob = this.queue.shift();
      const worker = this.workers[workerId];

      if (worker && !worker.busy) {
        this.startJob(
          worker,
          nextJob.file,
          nextJob.hash,
          nextJob.createThumbnail,
          nextJob.originalImg,
          nextJob,
        );
      }
    }
  }

  async waitForCompletion() {
    return new Promise((resolve) => {
      const checkInterval = setInterval(() => {
        const allFree = this.workers.every((w) => !w.busy);
        const queueEmpty = this.queue.length === 0;

        if (allFree && queueEmpty) {
          clearInterval(checkInterval);
          resolve();
        }
      }, 100);
    });
  }

  terminate() {
    this.workers.forEach((w) => {
      w.worker.terminate();
    });
    this.workers = [];
    this.queue = [];
    this.activeJobs.forEach((job) => {
      if (job.timeoutId) clearTimeout(job.timeoutId);
    });
    this.activeJobs.clear();
  }
}

const workerPool = new WorkerPool("/workers/imageProcessor.js");

// ðŸ”¥ Enhanced Retry Handler
class RetryHandler {
  constructor() {
    this.retryDelays = [500, 1000, 2000, 5000];
  }

  async executeWithRetry(fn, maxAttempts, context = {}) {
    let lastError;

    for (let attempt = 1; attempt <= maxAttempts; attempt++) {
      try {
        return await fn(attempt);
      } catch (error) {
        lastError = error;

        if (!this.isRetryableError(error) || attempt >= maxAttempts) {
          throw error;
        }

        const delay =
          this.retryDelays[Math.min(attempt - 1, this.retryDelays.length - 1)];
        console.warn(
          `âš ï¸ Retry ${attempt}/${maxAttempts}: ${context.fileName} (${delay}ms)`,
        );
        await this.sleep(delay);
      }
    }

    throw lastError;
  }

  isRetryableError(error) {
    const retryableStatuses = [408, 429, 500, 502, 503, 504];
    const retryableMessages = [
      "timeout",
      "network",
      "ECONNRESET",
      "ETIMEDOUT",
      "ENOTFOUND",
      "Bad Gateway",
      "Gateway Timeout",
      "Service Unavailable",
      "ERR_NETWORK",
      "ERR_FAILED",
    ];

    if (error.response && retryableStatuses.includes(error.response.status)) {
      return true;
    }

    const errorMessage = error.message?.toLowerCase() || "";
    return retryableMessages.some((msg) =>
      errorMessage.includes(msg.toLowerCase()),
    );
  }

  sleep(ms) {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }
}

const retryHandler = new RetryHandler();

// ðŸ”¥ MAIN UPLOADER CLASS
class SimpleUploader {
  constructor() {
    // Adaptive batch sizes based on file size
    this.batchSize = 20;
    this.concurrentUploads = 12;
    this.isCancelled = false;
    this.pendingPaths = new Set();
    this.failedPaths = new Set();

    this.uploadStats = {
      totalBytes: 0,
      uploadedBytes: 0,
      startTime: null,
      lastBytes: 0,
      lastTime: null,
      lastSpeedUpdate: null,
    };

    this.loadOrphanedFiles();
  }

  saveOrphanToStorage(path) {
    try {
      const existing = JSON.parse(
        localStorage.getItem(ORPHAN_STORAGE_KEY) || "[]",
      );
      if (!existing.includes(path)) {
        existing.push(path);
        localStorage.setItem(ORPHAN_STORAGE_KEY, JSON.stringify(existing));
      }
    } catch (e) {}
  }

  removeOrphanFromStorage(path) {
    try {
      const existing = JSON.parse(
        localStorage.getItem(ORPHAN_STORAGE_KEY) || "[]",
      );
      const filtered = existing.filter((p) => p !== path);
      localStorage.setItem(ORPHAN_STORAGE_KEY, JSON.stringify(filtered));
    } catch (e) {}
  }

  loadOrphanedFiles() {
    try {
      const orphans = JSON.parse(
        localStorage.getItem(ORPHAN_STORAGE_KEY) || "[]",
      );
      if (orphans.length > 0) {
        orphans.forEach((path) => this.pendingPaths.add(path));
        this.cleanupOrphaned();
      }
    } catch (e) {}
  }

  // Adaptive batch size based on file sizes
  calculateOptimalBatchSize(files) {
    const avgSizeMB =
      files.reduce((sum, f) => sum + f.size, 0) / files.length / 1024 / 1024;

    if (avgSizeMB > 30) return 5; // Large files
    if (avgSizeMB > 10) return 10; // Medium files
    return 20; // Small files
  }

  calculateOptimalConcurrency(files) {
    const avgSizeMB =
      files.reduce((sum, f) => sum + f.size, 0) / files.length / 1024 / 1024;

    if (avgSizeMB > 30) return 4; // Large files
    if (avgSizeMB > 10) return 8; // Medium files
    return 12; // Small files
  }

  async uploadFiles(files, callbacks) {
    const {
      setUploadTracker,
      uploadImages,
      user,
      uploadEventId,
      uploadAlbumId,
      originalImg,
      completeUpload,
    } = callbacks;

    this.isCancelled = false;
    const results = { successCount: 0, failedCount: 0 };

    // Start network monitoring
    realSpeedDetector.startMonitoring();

    // Calculate optimal settings based on file sizes
    this.batchSize = this.calculateOptimalBatchSize(files.map((f) => f.file));
    this.concurrentUploads = this.calculateOptimalConcurrency(
      files.map((f) => f.file),
    );

    const totalSizeMB = files.reduce((sum, f) => sum + (f.size || 0.5), 0);

    this.uploadStats = {
      totalBytes: totalSizeMB * 1048576,
      uploadedBytes: 0,
      startTime: Date.now(),
      lastBytes: 0,
      lastTime: Date.now(),
      lastSpeedUpdate: Date.now(),
    };

    try {
      for (let i = 0; i < files.length; i += this.batchSize) {
        if (this.isCancelled) {
          console.log("âŒ Upload cancelled by user");
          break;
        }

        const batchNum = Math.floor(i / this.batchSize) + 1;
        const batch = files.slice(i, i + this.batchSize);

        // STEP 1: Process images
        const processedBatch = await this.processBatchImages(
          batch,
          setUploadTracker,
          originalImg,
        );

        // Wait for all processing to complete
        await workerPool.waitForCompletion();

        // STEP 2: Upload files
        const retryAttempts = realSpeedDetector.currentSpeed.retryAttempts || 3;
        const uploadedBatch = await this.uploadBatch(
          processedBatch,
          { user, uploadEventId, uploadAlbumId, originalImg, retryAttempts },
          setUploadTracker,
        );

        // STEP 3: Save to database
        const successfulFiles = uploadedBatch.filter((r) => r.success);
        if (successfulFiles.length > 0) {
          try {
            await uploadImages(
              uploadAlbumId,
              successfulFiles.map((r) => r.metadata),
            );
            results.successCount += successfulFiles.length;

            // Cleanup orphan tracking
            successfulFiles.forEach((r) => {
              r.uploadedPaths.forEach((path) => {
                this.pendingPaths.delete(path);
                this.removeOrphanFromStorage(path);
              });
            });

            // Remove from UI
            successfulFiles.forEach((r) => {
              setUploadTracker((prev) => ({
                ...prev,
                files: prev.files.filter((f) => f.hash !== r.metadata.hash),
              }));
            });
          } catch (dbError) {
            console.error("âŒ DB save failed:", dbError);
            results.failedCount += successfulFiles.length;
          }
        }

        const failedFiles = uploadedBatch.filter((r) => !r.success);
        results.failedCount += failedFiles.length;

        // Update progress with real-time stats
        this.updateProgress(i, batch.length, files.length, setUploadTracker);
      }
    } finally {
      realSpeedDetector.stopMonitoring();
      setUploadTracker((prev) => ({
        ...prev,
        results: {
          successCount: results.successCount,
          failedCount: results.failedCount,
        },
      }));
      completeUpload(results);
      await this.cleanupOrphaned();
    }
  }

  updateProgress(currentIndex, batchSize, totalFiles, setUploadTracker) {
    const now = Date.now();
    const timeSinceLastUpdate = now - this.uploadStats.lastSpeedUpdate;

    // Update every 2 seconds for better performance
    if (timeSinceLastUpdate >= 2000) {
      const progressPercent = Math.round(
        ((currentIndex + batchSize) / totalFiles) * 100,
      );

      const timeDiff = (now - this.uploadStats.lastTime) / 1000;
      const bytesDiff =
        this.uploadStats.uploadedBytes - this.uploadStats.lastBytes;
      const instantSpeedMbps =
        bytesDiff > 0 && timeDiff > 0
          ? (bytesDiff * 8) / (timeDiff * 1000000)
          : 0;

      this.uploadStats.lastBytes = this.uploadStats.uploadedBytes;
      this.uploadStats.lastTime = now;
      this.uploadStats.lastSpeedUpdate = now;

      if (instantSpeedMbps > 0) {
        realSpeedDetector.recordProgress(
          this.uploadStats.uploadedBytes,
          this.uploadStats.startTime,
        );
      }

      const remainingBytes =
        this.uploadStats.totalBytes - this.uploadStats.uploadedBytes;
      const estimatedSeconds =
        remainingBytes > 0 && instantSpeedMbps > 0
          ? realSpeedDetector.estimateRemainingTime(remainingBytes)
          : 0;

      const speedIndicator = realSpeedDetector.getSpeedIndicator();

      setUploadTracker((prev) => ({
        ...prev,
        totalProgress: progressPercent,
        networkSpeed: speedIndicator.mbps,
        speedQuality: speedIndicator.quality,
        speedIcon: speedIndicator.icon,
        speedColor: speedIndicator.color,
        speedBars: speedIndicator.bars,
        estimatedTimeRemaining: Math.ceil(estimatedSeconds),
      }));
    } else {
      // Just update progress percentage
      const progressPercent = Math.round(
        ((currentIndex + batchSize) / totalFiles) * 100,
      );
      setUploadTracker((prev) => ({
        ...prev,
        totalProgress: progressPercent,
      }));
    }
  }

  async processBatchImages(batch, setUploadTracker, originalImg) {
    const limit = pLimit(
      Math.min(this.concurrentUploads, navigator.hardwareConcurrency || 4),
    );

    const results = await Promise.all(
      batch.map((fileData) =>
        limit(async () => {
          if (this.isCancelled) return null;

          try {
            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "processing",
              0,
            );

            const processed = await workerPool.processImage(
              fileData.file,
              fileData.hash,
              true,
              originalImg,
            );

            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "processed",
              10,
            );

            return { fileData, processed };
          } catch (error) {
            console.error(`âŒ Process failed: ${fileData.originalName}`, error);
            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "failed",
              0,
              error.message,
            );
            return null;
          }
        }),
      ),
    );

    return results.filter((r) => r !== null);
  }

  async uploadBatch(batch, config, setUploadTracker) {
    const limit = pLimit(this.concurrentUploads);

    return Promise.all(
      batch.map(({ fileData, processed }) =>
        limit(async () => {
          if (this.isCancelled) return { success: false, fileData };

          try {
            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "uploading",
              20,
            );

            const metadata = await retryHandler.executeWithRetry(
              async () => {
                return await this.uploadFileToB2(
                  fileData,
                  processed,
                  config,
                  setUploadTracker,
                );
              },
              config.retryAttempts,
              { fileName: fileData.originalName },
            );

            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "completed",
              100,
            );

            return {
              success: true,
              metadata,
              uploadedPaths: metadata.uploadedPaths,
            };
          } catch (error) {
            console.error(`âŒ Upload failed: ${fileData.originalName}`, error);
            this.updateFileStatus(
              setUploadTracker,
              fileData.hash,
              "failed",
              0,
              error.message,
            );
            return {
              success: false,
              fileData,
              error: error.message,
              uploadedPaths: error.uploadedPaths || [],
            };
          }
        }),
      ),
    );
  }

  async uploadFileToB2(fileData, processed, config, setUploadTracker) {
    const {
      originalBlob,
      thumbnailBlob,
      metadata: procMetadata,
      outputFormat,
      outputExt,
    } = processed;
    const { user, uploadEventId, uploadAlbumId, originalImg } = config;
    const basePath = `${user._id}/${uploadEventId}/${uploadAlbumId}/${fileData.hash}`;

    const uploadedPaths = [];
    const metadata = {
      baseFolderPath: basePath,
      hash: fileData.hash,
      originalName: fileData.originalName,
      uploadedBy: user._id,
    };

    try {
      const thumbnailPath = `optimized/${basePath}.${outputExt}`;
      const uploads = [];

      // Upload thumbnail using SmartFileUploader
      uploads.push(
        (async () => {
          const uploader = new SmartFileUploader(
            thumbnailBlob,
            thumbnailPath,
            outputFormat,
            (progress, bytesLoaded) => {
              const adjustedProgress = originalImg
                ? 20 + Math.floor(progress * 0.4)
                : 20 + Math.floor(progress * 0.8);
              this.updateFileStatus(
                setUploadTracker,
                fileData.hash,
                "uploading",
                adjustedProgress,
              );
              this.uploadStats.uploadedBytes += bytesLoaded;
            },
          );

          await uploader.upload();

          uploadedPaths.push(thumbnailPath);
          this.pendingPaths.add(thumbnailPath);
          this.saveOrphanToStorage(thumbnailPath);

          metadata.thumbnail = {
            width: procMetadata.thumbnail.width,
            height: procMetadata.thumbnail.height,
            folderName: "optimized",
            sizeInMB: procMetadata.thumbnail.sizeInMB,
            mimeType: outputFormat,
            path: thumbnailPath,
          };
        })(),
      );

      // Upload original if enabled
      if (originalImg) {
        const originalExt = fileData.file.type.split("/")[1] || "jpg";
        const originalPath = `originals/${basePath}.${originalExt}`;

        uploads.push(
          (async () => {
            const uploader = new SmartFileUploader(
              originalBlob,
              originalPath,
              fileData.file.type,
              (progress, bytesLoaded) => {
                const adjustedProgress = 60 + Math.floor(progress * 0.4);
                this.updateFileStatus(
                  setUploadTracker,
                  fileData.hash,
                  "uploading",
                  adjustedProgress,
                );
                this.uploadStats.uploadedBytes += bytesLoaded;
              },
            );

            await uploader.upload();

            uploadedPaths.push(originalPath);
            this.pendingPaths.add(originalPath);
            this.saveOrphanToStorage(originalPath);

            metadata.file = {
              width: procMetadata.original.width,
              height: procMetadata.original.height,
              folderName: "originals",
              sizeInMB: procMetadata.original.sizeInMB,
              mimeType: fileData.file.type,
              path: originalPath,
            };
          })(),
        );
      }

      // Wait for both uploads in parallel
      await Promise.all(uploads);

      if (!originalImg) {
        metadata.file = { ...metadata.thumbnail };
      }

      metadata.uploadedPaths = uploadedPaths;
      return metadata;
    } catch (error) {
      if (uploadedPaths.length > 0) {
        await this.cleanup(uploadedPaths);
      }
      error.uploadedPaths = uploadedPaths;
      throw error;
    }
  }

  updateFileStatus(setUploadTracker, hash, status, progress, error = null) {
    setUploadTracker((prev) => ({
      ...prev,
      files: prev.files.map((f) =>
        f.hash === hash ? { ...f, status, progress, error } : f,
      ),
    }));
  }

  async cleanup(paths) {
    if (!paths || paths.length === 0) return;

    try {
      await failedDeleteImagesFromB2(paths.map((p) => ({ Key: p })));

      paths.forEach((p) => {
        this.pendingPaths.delete(p);
        this.failedPaths.delete(p);
        this.removeOrphanFromStorage(p);
      });
    } catch (error) {
      console.error("âš ï¸ Cleanup failed:", error.message);
    }
  }

  async cleanupOrphaned() {
    const orphanedPaths = [
      ...Array.from(this.pendingPaths),
      ...Array.from(this.failedPaths),
    ];

    if (orphanedPaths.length > 0) {
      await this.cleanup(orphanedPaths);
    }

    this.pendingPaths.clear();
    this.failedPaths.clear();
  }

  cancel() {
    console.log("ðŸ›‘ Cancelling upload...");
    this.isCancelled = true;
    realSpeedDetector.stopMonitoring();
  }
}

export const simpleUploader = new SimpleUploader();
export {
  workerPool,
  realSpeedDetector,
  retryHandler,
  SmartFileUploader,
  UPLOAD_THRESHOLDS,
};
